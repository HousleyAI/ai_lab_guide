{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6e10203",
   "metadata": {},
   "source": [
    "# Run AI Inference using Python Code #\n",
    "- In this lab you will run some simple Python code to run inference against the Nvidia NIM using the OpenAI Python package\n",
    "- In earlier labs you have created containers that allow model inference. These will be the destination to which the Python script will send send API calls.  A responce will be returned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec24a3c",
   "metadata": {},
   "source": [
    "- Recall in previous lab you looked at the Service and Route objects of the NIM\n",
    "- The service address was used to access the NIM from within the cluster\n",
    "- The route will be required to access the NIM from outside the cluster\n",
    "- Note in the code below that the URL will be the route\n",
    "- The url will have a /v1 appended to identify conenction is to the API version 1 - this is required\n",
    "\n",
    "![YAML list: ](images/nim-route.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcfb94d",
   "metadata": {},
   "source": [
    "## Run Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8897845",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to install the pakages you would run the pip install command\n",
    "### It has already been installed onto your jumphosts\n",
    "\n",
    "'''\n",
    "\n",
    "!pip install openai\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90492fa3",
   "metadata": {},
   "source": [
    "The OpenAI Python library provides convenient access to the OpenAI REST API\n",
    "\n",
    "OpenAI provides an API (Application Programming Interface) that allows developers to easily access their AI models and integrate them into their own applications.\n",
    "\n",
    "https://platform.openai.com/docs/overview\n",
    "\n",
    "https://github.com/openai/openai-python\n",
    "\n",
    "The following code is used to access the NIM that has been installed and make query request of the AI model\n",
    "\n",
    "Ensure you have the kernel selected before you run this code\n",
    "\n",
    "Within the code is the question \"What is a GPU?\"  Feel free to change this a observe responces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decd303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "# openai is a package that contains the Python function calls to the model\n",
    "# https://openai.com/\n",
    "\n",
    "import httpx                \n",
    "# Required to ignore SSL verification\n",
    "\n",
    " \n",
    "client = OpenAI(base_url = \"https://nim-meta-llama-3-2-3b-instruct-ai-class.apps.ocp.ucsx.hl.dns/v1\", api_key=\"no-key-required\", http_client=httpx.Client(verify=False))\n",
    "# client is variable that creates an instance to the OpenAI class\n",
    "# the class provides all the necessary functionality to interact with the model\n",
    "# the client has connection details for the model as it has been passed the URL and key (albiet not required here)\n",
    "# the URL is the Openshift Route for the model container/pod instance\n",
    "\n",
    "'''\n",
    "client = OpenAI(base_url = \"http://localhost:8010/v1\", api_key=\"no-key-required\")\n",
    "# this line has the url if you are running the nim/model locally \n",
    "# the default port is 8000 but that confliced so was remapped to 8010\n",
    "'''\n",
    " \n",
    "responce  = client.chat.completions.create(\n",
    "  model=\"meta/llama-3.2-3b-instruct\",\n",
    "  messages=[{\"role\": \"user\", \"content\": \"What is a GPU?\"}],\n",
    "  temperature=0.5,\n",
    "  top_p=1,\n",
    "  max_tokens=1024\n",
    ")\n",
    "# this sends request to the models using the \"chat.completions.create\" method (within class OpenAI)\n",
    "\n",
    "# the \"messages\" parameter is the user defined prompt that is sent\n",
    "# note it is a dictionary defining \n",
    "#  the prompt type as user and \n",
    "#  the content being the quesiton \"What is a GPU\"\n",
    "\n",
    "# the returned value from the model is stored in the \"responce\" variable\n",
    "\n",
    "# the other parameters tune how the model is proces the request and how respond\n",
    "\n",
    "\n",
    "print(responce.choices[0].message.content)\n",
    "# the reponce is printed    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
