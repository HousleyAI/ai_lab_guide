{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbd3ac35",
   "metadata": {},
   "source": [
    "## VLLM Inference\n",
    "- During this task you will need to open the linux Terminal on your jumphost desktop as well as running commands in the notebook cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca2555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Logging into OpenShift #####\n",
    "### Set Student Number ###\n",
    "student_number = \"##\"      # Replace with your student number\n",
    "\n",
    "if student_number == \"##\":\n",
    "    raise ValueError(\"Please set your student number in the 'student_number' variable.\")\n",
    "\n",
    "### Login to OpenShift ###\n",
    "!oc login -u s{student_number} -p\"!@34QWer\" https://api.ocp.ucsx.hl.dns:6443 --insecure-skip-tls-verify\n",
    "!oc project ai-s{student_number}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c0c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the name of pod in your namespace ###\n",
    "!oc get pods\n",
    "\n",
    "# NAME                                           READY   STATUS      RESTARTS   AGE\n",
    "# vllm-deployment-74fb75bfb7-rngg2               1/1     Running     0          13m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37055b95",
   "metadata": {},
   "source": [
    "## Accessing the VLLM Deployment Pod\n",
    "- From you jumphost Desktop open 3 Terminal Sessions\n",
    "- In all three terminal sessions run the following command to access the VLLM Deployment Pod's bash shell:\n",
    "- The name will be different for you, use the pod name from the previous step\n",
    "```\n",
    "oc exec -it vllm-deployment-74fb75bfb7-rngg2 -- bash\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3847ba7e",
   "metadata": {},
   "source": [
    "## Terminal 1 - nvidia-smi monitoring\n",
    "- This command will show the GPU usage in your pod and update it every 2 seconds\n",
    "- Refer back to this terminal while ai inference in running\n",
    "- Run command:\n",
    "```\n",
    "watch nvidia-smi\n",
    "\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  NVIDIA L40                     On  |   00000000:3D:00.0 Off |                    0 |\n",
    "| N/A   54C    P0            110W /  300W |    5453MiB /  46068MiB |      0%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                              |\n",
    "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
    "|        ID   ID                                                               Usage      |\n",
    "|=========================================================================================|\n",
    "|  No running processes found                                                             |\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eadbed",
   "metadata": {},
   "source": [
    "## Terminal 2 - Start Inference Server\n",
    "- In this terminal you will start the VLLM inference server\n",
    "- Usually this will be done inside the command / args section of a kubernetes deployment yaml file\n",
    "- For this lab you will run the command directly in the terminal so you can see the startup sequence\n",
    "- This command has several important parameters:\n",
    "  - This will server the Qwen3 0.6B model\n",
    "  - Has a limited context window of 2k\n",
    "  - Set to only use 5% of the GPU\n",
    "- As this is a shared environment, Be sure to include max-model-len and gpu-memory-utilization parameters to limit resource usage as otherwise by default it will fill 95% of the VRAM\n",
    "\n",
    "- Run command:\n",
    "```\n",
    "vllm serve Qwen/Qwen3-0.6B --trust-remote-code --enable-chunked-prefill --max_num_batched_tokens 1024 --max-model-len 2048 --gpu-memory-utilization 0.05\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8830b312",
   "metadata": {},
   "source": [
    "Look through the logs as it stats - You will notice:\n",
    "- It downloads the model from the internet the first time you run it (which is saved in the persistent volume)\n",
    "- Loads the model into GPU memory\n",
    "- Starts the inference server and show all the URL endpoints that it servers\n",
    "\n",
    "```\n",
    "...\n",
    "(EngineCore_DP0 pid=854) INFO 11-17 14:43:12 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
    "model.safetensors: 100%|███████████████████████████████████████████████████████████████████████| 1.50G/1.50G [01:32<00:00, 16.2MB/s]\n",
    "(EngineCore_DP0 pid=854) INFO 11-17 14:44:45 [weight_utils.py:413] Time spent downloading weights for Qwen/Qwen3-0.6B: 93.483830 seconds\n",
    "(EngineCore_DP0 pid=854) INFO 11-17 14:44:46 [weight_utils.py:450] No model.safetensors.index.json found in remote.\n",
    "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
    "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.97it/s]\n",
    "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.96it/s]\n",
    "\n",
    "...\n",
    "(APIServer pid=553) INFO 11-17 14:45:24 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST\n",
    "(APIServer pid=553) INFO 11-17 14:45:24 [launcher.py:42] Route: /v1/chat/completions, Methods: POST\n",
    "(APIServer pid=553) INFO 11-17 14:45:24 [launcher.py:42] Route: /v1/completions, Methods: POST\n",
    "...\n",
    "(APIServer pid=553) INFO:     Started server process [553]\n",
    "(APIServer pid=553) INFO:     Waiting for application startup.\n",
    "(APIServer pid=553) INFO:     Application startup complete.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafdfbfc",
   "metadata": {},
   "source": [
    "## Return to Terminal 1 - nvidia-smi monitoring\n",
    "- You should see a new process down the bottom with how much VRAM is being used by it\n",
    "```\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  NVIDIA L40                     On  |   00000000:3D:00.0 Off |                    0 |\n",
    "| N/A   54C    P0            110W /  300W |    8878MiB /  46068MiB |      0%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                              |\n",
    "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
    "|        ID   ID                                                               Usage      |\n",
    "|=========================================================================================|\n",
    "|    0   N/A  N/A             854      C   VLLM::EngineCore                       3420MiB |   <----\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfa71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set the base URL variable for the VLLM inference server ###\n",
    "from openai import OpenAI\n",
    "import httpx                # Required to ignore SSL verification\n",
    "\n",
    "base_url = f\"https://vllm-route-ai-s{student_number}.apps.ocp.ucsx.hl.dns/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45938bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check Available Models being serverd by the OpenAI compatible API ###\n",
    "client = OpenAI(base_url=base_url, api_key=\"no-key-required\", http_client=httpx.Client(verify=False))\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "for model in models.data:\n",
    "    print(model.id)\n",
    "    \n",
    "    \n",
    "# Qwen/Qwen3-0.6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289b6725",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run Inference ###\n",
    "client = OpenAI(base_url=base_url, api_key=\"no-key-required\", http_client=httpx.Client(verify=False))\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"Qwen/Qwen3-0.6B\",\n",
    "  messages=[{\"role\": \"user\", \"content\": \"What is a GPU?\"}],\n",
    "  temperature=0.5,\n",
    "  top_p=1,\n",
    "  max_tokens=1024,\n",
    "  stream=True\n",
    ")\n",
    " \n",
    "for chunk in completion:\n",
    "  if chunk.choices[0].delta.content is not None:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cefa7f1",
   "metadata": {},
   "source": [
    "## While the inference is running look back at Terminal 1 - nvidia-smi monitoring\n",
    "- You should see the GPU Usage spike as inference is being performed\n",
    "- Its in the midde box on the right\n",
    "```\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  NVIDIA L40                     On  |   00000000:3D:00.0 Off |                    0 |\n",
    "| N/A   54C    P0            110W /  300W |    8878MiB /  46068MiB |      90%     Default |    <-----\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                              |\n",
    "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
    "|        ID   ID                                                               Usage      |\n",
    "|=========================================================================================|\n",
    "|    0   N/A  N/A             854      C   VLLM::EngineCore                       3420MiB |\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea373f8",
   "metadata": {},
   "source": [
    "## Look at Terminal 2 - Inference Server Logs\n",
    "- You will see the inference server logs showing the incoming request and the response being generated\n",
    "- It also includes a summary of the performance metrics for the request\n",
    "\n",
    "```\n",
    "(APIServer pid=553) INFO:     10.131.0.2:50816 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
    "(APIServer pid=553) INFO 11-17 14:54:25 [loggers.py:127] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 53.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f361d0",
   "metadata": {},
   "source": [
    "## Terminal 3 - Running a benchmark test in the VLLM Deployment Pod\n",
    "- This benchmark runs 100 prompt tests (10 at a time) and returns a summary\n",
    "- While its running look at the Terminal 1 - nvidia-smi and look at the GPU usage again\n",
    "- This requires that the vllm still be running in Terminal 2\n",
    "Note:\tIf other students are also actively using the GPU, the time taken will be longer as the GPUs are time-sliced and gpu-utilisation is shared between users\n",
    "\n",
    "Run Command:\n",
    "```\n",
    "vllm bench serve --backend vllm --model Qwen/Qwen3-0.6B --endpoint /v1/completions --dataset-name random --num-prompts 100 --max-concurrency 10\n",
    "```\n",
    "\n",
    "Expected Output:\n",
    "```\n",
    "============ Serving Benchmark Result ============\n",
    "Successful requests:                     100\n",
    "Maximum request concurrency:             10\n",
    "Benchmark duration (s):                  7.70\n",
    "Total input tokens:                      102017\n",
    "Total generated tokens:                  11915\n",
    "Request throughput (req/s):              12.99\n",
    "Output token throughput (tok/s):         1547.38\n",
    "Peak output token throughput (tok/s):    1675.00\n",
    "Peak concurrent requests:                29.00\n",
    "Total Token throughput (tok/s):          14796.16\n",
    "---------------Time to First Token----------------\n",
    "Mean TTFT (ms):                          51.04\n",
    "Median TTFT (ms):                        48.34\n",
    "P99 TTFT (ms):                           121.48\n",
    "-----Time per Output Token (excl. 1st token)------\n",
    "Mean TPOT (ms):                          5.99\n",
    "Median TPOT (ms):                        5.85\n",
    "P99 TPOT (ms):                           11.92\n",
    "---------------Inter-token Latency----------------\n",
    "Mean ITL (ms):                           5.80\n",
    "Median ITL (ms):                         5.50\n",
    "P99 ITL (ms):                            12.95\n",
    "==================================================\n",
    "```\n",
    "\n",
    "\n",
    "### On Terminal 2 - Inference Server Logs\n",
    "- You will see multiple incoming requests and some performance metrics being displayed as the benchmark runs\n",
    "```\n",
    "(APIServer pid=553) INFO:     127.0.0.1:55748 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
    "(APIServer pid=553) INFO:     127.0.0.1:55760 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
    "(APIServer pid=553) INFO:     127.0.0.1:55696 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
    "(APIServer pid=553) INFO:     127.0.0.1:55778 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
    "(APIServer pid=553) INFO 11-17 15:05:15 [loggers.py:127] Engine 000: Avg prompt throughput: 7744.3 tokens/s, Avg generation throughput: 858.3 tokens/s, Running: 4 reqs, Waiting: 6 reqs, GPU KV cache usage: 86.4%, Prefix cache hit rate: 29.5%\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
